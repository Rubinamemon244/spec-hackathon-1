---
sidebar_position: 1
title: "Voice-to-Action Interfaces"
---

# Voice-to-Action Interfaces

## Overview

In this chapter, we'll explore how to create voice-controlled interfaces for humanoid robots using OpenAI Whisper for speech recognition. Voice interfaces provide a natural way for humans to interact with robots, enabling hands-free control and intuitive communication.

## Learning Objectives

By the end of this chapter, you will be able to:
- Integrate OpenAI Whisper for speech recognition in robotics applications
- Process voice commands and map them to robot actions
- Design robust voice command grammars for humanoid control
- Handle speech recognition errors and uncertainties
- Implement voice feedback mechanisms for user interaction

## Prerequisites

Before starting this chapter, ensure you have:
- Basic understanding of ROS 2 concepts and node communication
- Familiarity with audio processing concepts
- OpenAI API access (for Whisper service) or local Whisper installation
- Basic Python programming skills

## Introduction to Voice Control in Robotics

Voice control represents a natural evolution in human-robot interaction, allowing users to command robots using natural language. For humanoid robots, voice interfaces can significantly enhance usability and accessibility, especially in scenarios where manual control is impractical.

### Key Benefits of Voice Control:
- **Natural Interaction**: Humans naturally communicate through speech
- **Hands-Free Operation**: Particularly useful when hands are occupied
- **Accessibility**: Enables interaction for users with mobility limitations
- **Multimodal Integration**: Combines with visual and haptic feedback

## Setting Up OpenAI Whisper for Robot Control

OpenAI Whisper is a state-of-the-art speech recognition model that can transcribe speech to text with high accuracy. We'll use it to convert voice commands into actionable text for our humanoid robot.

### Installation and Configuration

First, install the required dependencies:

```bash
pip install openai-whisper
pip install pyaudio
pip install speechrecognition
```

For ROS 2 integration, we'll also need:

```bash
pip install rclpy
```

### Basic Whisper Integration

Here's a basic example of integrating Whisper with your robot system:

```python
import whisper
import rospy
from std_msgs.msg import String
import pyaudio
import wave
import numpy as np

class VoiceControlNode:
    def __init__(self):
        # Initialize ROS node
        rospy.init_node('voice_control_node')

        # Publisher for voice commands
        self.command_publisher = rospy.Publisher('/voice_command', String, queue_size=10)

        # Load Whisper model
        self.model = whisper.load_model("base.en")  # or "small", "medium", "large"

        # Audio recording parameters
        self.audio_format = pyaudio.paInt16
        self.channels = 1
        self.rate = 44100
        self.chunk = 1024
        self.record_seconds = 5

        print("Voice control system initialized")

    def record_audio(self):
        """Record audio from microphone"""
        audio = pyaudio.PyAudio()

        stream = audio.open(
            format=self.audio_format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        print("Recording...")
        frames = []

        for _ in range(0, int(self.rate / self.chunk * self.record_seconds)):
            data = stream.read(self.chunk)
            frames.append(data)

        print("Recording finished")

        stream.stop_stream()
        stream.close()
        audio.terminate()

        # Save to WAV file temporarily
        wf = wave.open("/tmp/recording.wav", 'wb')
        wf.setnchannels(self.channels)
        wf.setsampwidth(audio.get_sample_size(self.audio_format))
        wf.setframerate(self.rate)
        wf.writeframes(b''.join(frames))
        wf.close()

        return "/tmp/recording.wav"

    def transcribe_speech(self, audio_file):
        """Transcribe speech using Whisper"""
        result = self.model.transcribe(audio_file)
        return result["text"]

    def process_voice_command(self, text):
        """Process transcribed text and generate robot commands"""
        text = text.lower().strip()

        # Define command patterns
        if "move forward" in text:
            return "MOVE_FORWARD"
        elif "move backward" in text:
            return "MOVE_BACKWARD"
        elif "turn left" in text:
            return "TURN_LEFT"
        elif "turn right" in text:
            return "TURN_RIGHT"
        elif "stop" in text:
            return "STOP"
        elif "wave" in text:
            return "WAVE_HAND"
        elif "dance" in text:
            return "PERFORM_DANCE"
        else:
            return f"UNKNOWN_COMMAND: {text}"

    def run(self):
        """Main loop for voice control"""
        rate = rospy.Rate(1)  # Check for commands once per second

        while not rospy.is_shutdown():
            # Record audio
            audio_file = self.record_audio()

            # Transcribe speech
            transcribed_text = self.transcribe_speech(audio_file)
            print(f"Transcribed: {transcribed_text}")

            # Process command
            command = self.process_voice_command(transcribed_text)
            print(f"Command: {command}")

            # Publish command
            msg = String()
            msg.data = command
            self.command_publisher.publish(msg)

            rate.sleep()

if __name__ == '__main__':
    node = VoiceControlNode()
    try:
        node.run()
    except rospy.ROSInterruptException:
        pass
```

## Advanced Voice Command Processing

For more sophisticated voice control, consider implementing:

### 1. Intent Recognition with Natural Language Processing

```python
import spacy
from collections import defaultdict

class AdvancedVoiceProcessor:
    def __init__(self):
        # Load spaCy model for NLP
        self.nlp = spacy.load("en_core_web_sm")

        # Define command patterns
        self.action_patterns = {
            "MOVEMENT": ["go", "move", "walk", "step"],
            "DIRECTION": ["forward", "backward", "left", "right", "up", "down"],
            "GESTURE": ["wave", "nod", "shake", "point", "clap"],
            "SPEAK": ["say", "speak", "tell", "announce"]
        }

    def extract_intent(self, text):
        """Extract intent and entities from voice command"""
        doc = self.nlp(text)

        intent = {
            "action": None,
            "direction": None,
            "distance": None,
            "entities": []
        }

        for token in doc:
            if token.text.lower() in self.action_patterns["MOVEMENT"]:
                intent["action"] = "MOVEMENT"
            elif token.text.lower() in self.action_patterns["DIRECTION"]:
                intent["direction"] = token.text.upper()
            elif token.text.lower() in self.action_patterns["GESTURE"]:
                intent["action"] = "GESTURE"

        # Extract distance/quantities
        for ent in doc.ents:
            if ent.label_ in ["CARDINAL", "QUANTITY"]:
                intent["distance"] = ent.text

        return intent
```

### 2. Confidence-Based Command Filtering

```python
def validate_command(self, transcription_result):
    """Validate command based on confidence and grammar"""
    text = transcription_result["text"]
    confidence = transcription_result.get("avg_logprob", -1.0)

    # Define minimum confidence threshold
    if confidence < -0.5:  # Adjust based on testing
        return False, "Low confidence transcription"

    # Check if command matches expected patterns
    valid_commands = [
        r"move\s+(forward|backward|left|right)",
        r"(turn|rotate)\s+(left|right)",
        r"stop",
        r"wave(\s+hand)?",
        r"dance",
        r"introduce\s+yourself"
    ]

    import re
    for pattern in valid_commands:
        if re.search(pattern, text.lower()):
            return True, "Valid command"

    return False, "Command does not match expected patterns"
```

## Real-Time Voice Processing

For real-time applications, implement continuous listening with wake word detection:

```python
import threading
import queue

class RealTimeVoiceController:
    def __init__(self):
        self.audio_queue = queue.Queue()
        self.command_queue = queue.Queue()
        self.result_queue = queue.Queue()
        self.is_listening = False
        self.wake_word = "hey robot"

        # Initialize Whisper model
        self.model = whisper.load_model("base.en")

        # Setup audio stream
        self.audio = pyaudio.PyAudio()
        self.stream = self.audio.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,  # Lower rate for real-time processing
            input=True,
            frames_per_buffer=1024,
            stream_callback=self.audio_callback
        )

    def audio_callback(self, in_data, frame_count, time_info, status):
        """Callback for real-time audio processing"""
        self.audio_queue.put(in_data)
        return (in_data, pyaudio.paContinue)

    def listen_thread(self):
        """Thread for continuous listening"""
        buffer = b""
        buffer_size = 16000 * 2  # 1 second of audio at 16kHz, 16-bit

        while self.is_listening:
            try:
                chunk = self.audio_queue.get(timeout=0.1)
                buffer += chunk

                # Process when we have enough audio
                if len(buffer) > buffer_size:
                    # Check for wake word in buffer
                    # Process with Whisper when wake word detected
                    pass
            except queue.Empty:
                continue

    def start_listening(self):
        """Start real-time voice processing"""
        self.is_listening = True
        self.listen_thread = threading.Thread(target=self.listen_thread)
        self.listen_thread.start()
```

## Integration with Isaac and Nav2

To integrate voice commands with navigation and movement:

```python
import rclpy
from geometry_msgs.msg import Twist
from nav2_msgs.action import NavigateToPose
from rclpy.action import ActionClient

class VoiceNavigationController:
    def __init__(self):
        self.node = rclpy.create_node('voice_navigation_controller')
        self.cmd_vel_pub = self.node.create_publisher(Twist, '/cmd_vel', 10)
        self.nav_client = ActionClient(self.node, NavigateToPose, 'navigate_to_pose')

    def execute_navigation_command(self, intent):
        """Execute navigation based on voice command intent"""
        if intent["action"] == "MOVEMENT":
            twist = Twist()

            if intent["direction"] == "FORWARD":
                twist.linear.x = 0.5
            elif intent["direction"] == "BACKWARD":
                twist.linear.x = -0.5
            elif intent["direction"] == "LEFT":
                twist.angular.z = 0.5
            elif intent["direction"] == "RIGHT":
                twist.angular.z = -0.5

            self.cmd_vel_pub.publish(twist)
```

## Practical Exercise: Voice-Controlled Navigation

### Objective
Create a voice-controlled navigation system for your Isaac-enabled humanoid robot.

### Steps:
1. Set up Whisper for speech recognition
2. Create a voice command parser
3. Map voice commands to robot movements
4. Test navigation in Isaac simulation

### Expected Outcome
A humanoid robot that responds to voice commands like "Move forward", "Turn left", etc.

## Troubleshooting Common Issues

### 1. Audio Quality Problems
- **Issue**: Poor transcription accuracy
- **Solution**: Use noise cancellation, improve microphone placement, or use audio preprocessing

### 2. Latency Issues
- **Issue**: Delay between speaking and robot response
- **Solution**: Optimize model size, use streaming recognition, or implement edge processing

### 3. Wake Word Detection
- **Issue**: System responding to unintended sounds
- **Solution**: Implement proper wake word detection with confidence thresholds

## Summary

Voice-to-action interfaces provide an intuitive way to control humanoid robots. By leveraging OpenAI Whisper's powerful speech recognition capabilities, we can create natural interaction experiences that bridge human language and robot actions. The key to success lies in proper audio processing, command validation, and seamless integration with the robot's control systems.

In the next chapter, we'll explore how to use Large Language Models for cognitive planning and task decomposition.